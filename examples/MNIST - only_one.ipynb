{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class PlotHelper():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._f = None\n",
    "        self._ax = None\n",
    "        self.kvals = defaultdict(list)\n",
    "\n",
    "    def add(self, **kval):\n",
    "        for k, v in kval.items():\n",
    "            self.kvals[k].append(v)\n",
    "\n",
    "    @property\n",
    "    def fig(self):\n",
    "        if self._f is None:\n",
    "            self.new()\n",
    "        return self._f\n",
    "\n",
    "    @property\n",
    "    def ax(self):\n",
    "        if self._ax is None:\n",
    "            self.new()\n",
    "        return self._ax\n",
    "\n",
    "    def new(self):\n",
    "        self._f, self._ax = plt.subplots(1,1)\n",
    "        plt.ion()\n",
    "        self.fig.show()\n",
    "\n",
    "    def show(self):\n",
    "        names = []\n",
    "        self.ax.clear()\n",
    "        for k, v in self.kvals.items():\n",
    "            names.append(k)\n",
    "            self.ax.plot(v)\n",
    "        self.ax.legend(names)\n",
    "        self.fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_balanced_subset(dataset, n_per_class):\n",
    "    classes = dataset.targets.unique()\n",
    "    indices = [(dataset.targets == c).nonzero(as_tuple=False) [:n_per_class] for c in classes]\n",
    "    indices = torch.stack(indices).flatten()\n",
    "    return torch.utils.data.Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x.reshape(10,2)\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch, cons=None, plot_loss=None):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        output = output.reshape(10,10)\n",
    "        \n",
    "        loss = F.cross_entropy(output[:1], target)\n",
    "        closs = cons(output) if cons else torch.tensor(0).to(device)\n",
    "        \n",
    "        loss += closs\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            output = output.reshape(10,10)\n",
    "            # sum up batch loss\n",
    "            test_loss += F.cross_entropy(output[:1], target, reduction='sum').item()\n",
    "            \n",
    "            # get the index of the max log-probability\n",
    "            pred = output[:1].argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pylon.constraint import constraint\n",
    "from pylon.brute_force_solver import SatisfactionBruteForceSolver\n",
    "from pylon.sampling_solver import SamplingSolver, WeightedSamplingSolver\n",
    "\n",
    "def only_one(x):\n",
    "    return x.sum(dim=-1) == 1\n",
    "\n",
    "class Args:\n",
    "    batch_size = 1\n",
    "    test_batch_size = 1000\n",
    "    epochs = 1\n",
    "    lr = 1.0\n",
    "    gamma = 0.7\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    use_cuda = False\n",
    "    n_per_class = 100\n",
    "    \n",
    "# Plotting\n",
    "plot_loss = PlotHelper()\n",
    "    \n",
    "args = Args()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'batch_size': args.batch_size}\n",
    "if args.use_cuda:\n",
    "    kwargs.update({'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True},\n",
    "                 )\n",
    "\n",
    "# Prepare dataset transformations\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "# Load train and test splits\n",
    "train_split = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "test_split = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "\n",
    "# Sample a balanced subset of the train set\n",
    "train_split = sample_balanced_subset(train_split, args.n_per_class)\n",
    "\n",
    "# Create train, validation and test data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_split,**kwargs)\n",
    "test_loader  = torch.utils.data.DataLoader(test_split, **kwargs)\n",
    "\n",
    "# Constraint to be applied on unlabeled data\n",
    "only_one_constraint = constraint(only_one, SatisfactionBruteForceSolver())\n",
    "# only_one_constraint = constraint(only_one, WeightedSamplingSolver(num_samples=200))\n",
    "# only_one_constraint = constraint(only_one, SamplingSolver(num_samples=200))\n",
    "\n",
    "# Move model to correct device\n",
    "model = Net().to(device)\n",
    "\n",
    "# Set up optimizer\n",
    "# optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/hanliying/Documents/UCLA/Research/Neurosym_proj/pylon-master/examples/../pylon/brute_force_solver.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indices = torch.stack([torch.tensor(data=self.cond(*sample), dtype=torch.bool) if kwargs == {}\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.33it/s]\n",
      "/var/folders/kk/zn19ckhn56s6kwjrwp70f8kc0000gp/T/ipykernel_2625/1932012216.py:36: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0397, -0.6701],\n",
      "         [-0.3310,  0.0271],\n",
      "         [ 0.2637, -1.0437],\n",
      "         [ 0.5965, -1.7663],\n",
      "         [ 0.0142, -0.1075],\n",
      "         [ 0.3138,  0.4218],\n",
      "         [ 0.8880, -1.2280],\n",
      "         [ 1.1796, -0.5855],\n",
      "         [ 0.8292, -0.6835],\n",
      "         [ 0.8635,  0.1001]]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([[[-1.0397, -0.6701],\n",
      "         [-0.3310,  0.0271],\n",
      "         [ 0.2637, -1.0437],\n",
      "         [ 0.5965, -1.7663],\n",
      "         [ 0.0142, -0.1075],\n",
      "         [ 0.3138,  0.4218],\n",
      "         [ 0.8880, -1.2280],\n",
      "         [ 1.1796, -0.5855],\n",
      "         [ 0.8292, -0.6835],\n",
      "         [ 0.8635,  0.1001]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# # Plotting\n",
    "plot_loss = PlotHelper()\n",
    "#range 500\n",
    "for i in tqdm(range(1)):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    output = model(X)\n",
    "    loss = F.cross_entropy(output[:,1].reshape(1,10), y)\n",
    "    closs = only_one_constraint(output.unsqueeze(0)) # Expects tensors of shape: batch_size x ... x num_classes\n",
    "    \n",
    "    total_loss = loss + closs\n",
    "    \n",
    "    plot_loss.add(loss=loss.data, closs=closs.data, )\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "plot_loss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.softmax(model(X), dim=-1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7a719078278c4492d805b55cadf911a94c7633b48007fee7a7c47218b923b9ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
