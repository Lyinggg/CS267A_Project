{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VGOolXKczuN5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liying/miniconda3/envs/pytorch-gpu/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Raw Data to Generate Primitive Activitiy Dataset\n",
    "The data we use here is from the Third Nurse Care Activity Recognition Challenge. It contains 3-dimensional accelerometer data from 12 subjects performing 27 different activities. We have filtered out noisy data and stored them in 'data_full.json', but further preprocessing is necessary since each activity data has arbitrary length. We are going to segment each data with the same lenghth, and choose the activity classes with balanced training data size to be the primitive activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "44PWuZuJW65b"
   },
   "outputs": [],
   "source": [
    "'''Read file and convert string to Python dict'''\n",
    "\n",
    "with open('./complex_event/data_full.json','r') as f:\n",
    "    load_dict = json.load(f)\n",
    "    # print(load_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qLFVzetdOalU"
   },
   "outputs": [],
   "source": [
    "'''Segment the raw data into same time length'''\n",
    "\n",
    "SEGMENT_SIZE = 20\n",
    "SLIDING_SIZE = SEGMENT_SIZE\n",
    "\n",
    "'''Choose primitive activitiy class indexed by target_label'''\n",
    "# 4: Excretion, 2: Nighttime user response, 12: Meal/medication, 9: Morning care, \n",
    "target_label = [4, 2, 12, 9]\n",
    "\n",
    "'''Rearrange class label'''\n",
    "# 0: Excretion, 1: Nighttime user response, 2: Meal/medication, 3: Morning care\n",
    "label_map = {4:0, 2:1, 12:2, 9:3}\n",
    "\n",
    "n_class = len(target_label)\n",
    "n_data_per_class = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25336,
     "status": "ok",
     "timestamp": 1664774080162,
     "user": {
      "displayName": "LIYING HAN",
      "userId": "12884455188191918408"
     },
     "user_tz": 420
    },
    "id": "66n2hnt3cQ4I",
    "outputId": "280b9972-e996-4a1a-88d1-edf9798327ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43542\n",
      "{0: 17684, 1: 18268, 3: 2948, 2: 4642}\n",
      "Activity 4 :\n",
      "The range of data length is ( 20 , 20 )\n",
      "The distribution of data length is Counter({20: 17684})\n",
      "Activity 2 :\n",
      "The range of data length is ( 20 , 20 )\n",
      "The distribution of data length is Counter({20: 18268})\n",
      "Activity 9 :\n",
      "The range of data length is ( 20 , 20 )\n",
      "The distribution of data length is Counter({20: 2948})\n",
      "Activity 12 :\n",
      "The range of data length is ( 20 , 20 )\n",
      "The distribution of data length is Counter({20: 4642})\n",
      "{3: 491, 1: 485, 2: 512, 0: 512}\n",
      "(6000, 3, 20)\n",
      "(6000,)\n",
      "(2000, 3, 20)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "'''Generate primitive activity data'''\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "x_prim_data = []\n",
    "y_prim_data = []\n",
    "time_period = set()\n",
    "activity_len = {}\n",
    "for item in load_dict:\n",
    "    x, y, start, finish = item['acc'], item['old_label'], item['start'], item['finish']\n",
    "    counter = dict(Counter(y_prim_data))\n",
    "    if y in target_label and (start, finish) not in time_period and len(x) >= SEGMENT_SIZE:\n",
    "        # Check duplicates\n",
    "        time_period.add((start, finish))\n",
    "        # Divide y into segements of length SEGMENT_SIZE\n",
    "        # TODO: Here just discard segments with length smaller than SEGMENT_SIZE, we can use imputation later to keep them\n",
    "        if SLIDING_SIZE == 0:\n",
    "            raise Exception(\"SLIDING_SIZE cannot be zero\")\n",
    "        num = int((len(x) - SEGMENT_SIZE)/SLIDING_SIZE) + 1\n",
    "        for i in range(num):\n",
    "            x_i = x[i * SLIDING_SIZE:i * SLIDING_SIZE + SEGMENT_SIZE]\n",
    "            # y_i = y\n",
    "            y_i = label_map[y]\n",
    "            x_prim_data.append(x_i)\n",
    "            y_prim_data.append(y_i)\n",
    "            \n",
    "            # count the number of data with different length for each activity\n",
    "            length = len(x_i)\n",
    "            if y not in activity_len:\n",
    "                activity_len[y] = [length]\n",
    "            else:\n",
    "                activity_len[y].append(length)\n",
    "                \n",
    "print(len(x_prim_data))\n",
    "result = dict(Counter(y_prim_data))\n",
    "print(result)\n",
    "\n",
    "\n",
    "# count the number of data with different length for each activity\n",
    "for key, val in activity_len.items():\n",
    "    n = Counter(val)\n",
    "    print(\"Activity\",key,\":\")\n",
    "    print(\"The range of data length is (\", min(val), \",\", max(val), \")\")\n",
    "    print(\"The distribution of data length is\", n)\n",
    "\n",
    "temp = list(zip(x_prim_data, y_prim_data))\n",
    "random.shuffle(temp)\n",
    "res1, res2 = zip(*temp)\n",
    "# res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "x_prim_data, y_prim_data = list(res1), list(res2)\n",
    "\n",
    "x_prim_data = np.array(x_prim_data).transpose((0,2,1))\n",
    "x_prim_data = x_prim_data.astype(np.float32)\n",
    "y_prim_data = np.array(y_prim_data)\n",
    "\n",
    "# balance data set of different activities\n",
    "x_prim_data_temp = np.zeros([n_data_per_class, x_prim_data.shape[1], x_prim_data.shape[2]])\n",
    "y_prim_data_temp = np.zeros([n_data_per_class, 1])\n",
    "for i in range(n_class):\n",
    "    idx = np.where(y_prim_data==i)[0]\n",
    "    idx = np.random.choice(idx, n_data_per_class, replace=False)\n",
    "    if i == 0:\n",
    "        x_prim_data_temp = x_prim_data[idx] \n",
    "        y_prim_data_temp = y_prim_data[idx] \n",
    "    else:\n",
    "        x_prim_data_temp = np.concatenate([x_prim_data_temp,x_prim_data[idx]],axis=0)\n",
    "        y_prim_data_temp = np.concatenate([y_prim_data_temp,y_prim_data[idx]],axis=0)\n",
    "\n",
    "x_prim_data = x_prim_data_temp\n",
    "y_prim_data = y_prim_data_temp\n",
    "\n",
    "# shuffle again\n",
    "temp = list(zip(x_prim_data, y_prim_data))\n",
    "random.shuffle(temp)\n",
    "res1, res2 = zip(*temp)\n",
    "# res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "x_prim_data, y_prim_data = list(res1), list(res2)\n",
    "\n",
    "x_prim_data = np.array(x_prim_data)\n",
    "x_prim_data.astype(np.float32)\n",
    "y_prim_data = np.array(y_prim_data)\n",
    "\n",
    "# split some data to test set\n",
    "x_prim_test = x_prim_data[-int(n_data_per_class/4*n_class):]\n",
    "x_prim_train = x_prim_data[:-int(n_data_per_class/4*n_class)]\n",
    "y_prim_test = y_prim_data[-int(n_data_per_class/4*n_class):]\n",
    "y_prim_train = y_prim_data[:-int(n_data_per_class/4*n_class)]\n",
    "\n",
    "print(dict(Counter(y_prim_test)))\n",
    "\n",
    "print(x_prim_train.shape)\n",
    "print(y_prim_train.shape)\n",
    "print(x_prim_test.shape)\n",
    "print(y_prim_test.shape)\n",
    "\n",
    "x_prim_train = torch.from_numpy(x_prim_train)\n",
    "y_prim_train = torch.from_numpy(y_prim_train)\n",
    "x_prim_test = torch.from_numpy(x_prim_test)\n",
    "y_prim_test = torch.from_numpy(y_prim_test)\n",
    "\n",
    "# print(x_prim_train.shape)\n",
    "# print(y_prim_train.shape)\n",
    "# print(x_prim_test.shape)\n",
    "# print(y_prim_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1664774080162,
     "user": {
      "displayName": "LIYING HAN",
      "userId": "12884455188191918408"
     },
     "user_tz": 420
    },
    "id": "A3HJjb23jk5x",
    "outputId": "57482925-0a21-4d75-925f-ca8d311bfc20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 3, 20)\n",
      "(2000, 3, 20)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(x_prim_train).shape)\n",
    "print(np.array(x_prim_test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Modules to Calssify Primitive Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YRYm4W1Ga_kO"
   },
   "outputs": [],
   "source": [
    "'''Define Neural Architecture'''\n",
    "\n",
    "from torch.nn.modules.activation import Softmax\n",
    "from torch.nn.modules import dropout\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, n_class, drop_out=0.5):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, padding='same'),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3),\n",
    "            nn.BatchNorm1d(num_features=8),\n",
    "            nn.Dropout(drop_out),\n",
    "  \n",
    "            # nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding='same'),\n",
    "            # nn.LeakyReLU(),\n",
    "            # nn.MaxPool1d(kernel_size=3, stride=3),\n",
    "            # nn.BatchNorm1d(num_features=16),\n",
    "            # nn.Dropout(drop_out)\n",
    "        )\n",
    "  \n",
    "        self.lstm = nn.LSTM(input_size=6, hidden_size=8, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=128, out_features=self.n_class),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # print(x.shape)\n",
    "        x, _ = self.lstm(x)\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Avl_JA3G5vBN"
   },
   "outputs": [],
   "source": [
    "'''Baseline training on neural module directly (not used in Pylon implementation)'''\n",
    "\n",
    "def train(model, x_train, y_train, epoch, batch_size):\n",
    "    batch_num = int(np.ceil(x_train.shape[0] / batch_size))\n",
    "    for k in range(epoch):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for i in range(batch_num):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            if i == batch_num - 1:\n",
    "                inputs, labels = x_train[i * batch_size:], y_train[i * batch_size:]\n",
    "            else:\n",
    "                inputs, labels = x_train[i * batch_size:(i + 1) * batch_size], y_train[i * batch_size:(i + 1) * batch_size]\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "  \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            running_acc += (predicted == labels).sum().item() / batch_size\n",
    "            if i % 100 == 99:    # print every 100 mini-batches\n",
    "                print(f'[{k + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f} acc: {running_acc / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "                running_acc = 0.0\n",
    "\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H1uju-St92n9"
   },
   "outputs": [],
   "source": [
    "'''Evaluation function used for primitive activity classification'''\n",
    "\n",
    "def evaluate(model, x_test, y_test):\n",
    "    correct = 0\n",
    "    total = y_test.shape[0]\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = x_test, y_test\n",
    "        outputs = model(inputs)\n",
    "        _, predict = torch.max(outputs.data, 1)\n",
    "        correct = (predict == labels).sum().item() / total\n",
    "  \n",
    "    print('Accuracy of the network on the test data: %d %%' % (\n",
    "      100 * correct))\n",
    "    \n",
    "    class_correct = list(0. for i in range(model.n_class))\n",
    "    class_total = list(0. for i in range(model.n_class))\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = x_test, y_test\n",
    "        outputs = model(inputs)\n",
    "        _, predict = torch.max(outputs.data, 1)\n",
    "        c = (predict == labels).squeeze()\n",
    "        for i in range(labels.shape[0]):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "  \n",
    "    print(class_total)\n",
    "    for i in range(model.n_class):\n",
    "        print('Accuracy of activity %2s : %2d %%' % (\n",
    "        i, 100 * class_correct[i] / class_total[i]))\n",
    "    return [predict, correct]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define FSM for complex events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n",
      "False True\n"
     ]
    }
   ],
   "source": [
    "'''Build FSM class'''\n",
    "\n",
    "class FSM:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "        self.init_state = 'init'\n",
    "        self.final_state = 'final'\n",
    "        \n",
    "    def state_transition(curr_state, curr_input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def traverse(self, curr_state, input_sequence):\n",
    "        if curr_state == self.final_state:\n",
    "            return True\n",
    "        if len(input_sequence) == 0:\n",
    "            return False\n",
    "        curr_input = input_sequence[0]\n",
    "        next_state = self.state_transition(curr_state, curr_input)\n",
    "        return self.traverse(next_state, input_sequence[1:])\n",
    "\n",
    "    def check(self, input_sequence):\n",
    "        return self.traverse(self.init_state, input_sequence)\n",
    "\n",
    "\n",
    "class Event0(FSM):\n",
    "    def __init__(self):\n",
    "        super().__init__(label=0)\n",
    "\n",
    "    def state_transition(self, curr_state, curr_input):\n",
    "        if curr_state == 'init':\n",
    "            if curr_input == 1:\n",
    "                next_state = 's1'\n",
    "            else:\n",
    "                next_state = 'init'\n",
    "        elif curr_state == 's1':\n",
    "            if curr_input == 3:\n",
    "                next_state = 'final'\n",
    "            else:\n",
    "                next_state = 's1'\n",
    "        elif curr_state == 'final':\n",
    "            if curr_input == 3:\n",
    "                next_state == 'final'\n",
    "            else:\n",
    "                next_state == 'init'\n",
    "        else:\n",
    "            raise Exception(f'State \"{curr_state}\" is not defined.')\n",
    "        # print(next_state)\n",
    "        return next_state\n",
    "\n",
    "class Event1(FSM):\n",
    "    def __init__(self):\n",
    "        super().__init__(label=1)\n",
    "\n",
    "    def state_transition(self, curr_state, curr_input):\n",
    "        if curr_state == 'init':\n",
    "            if curr_input == 0:\n",
    "                next_state = 's1'\n",
    "            else:\n",
    "                next_state = 'init'\n",
    "        elif curr_state == 's1':\n",
    "            if curr_input == 2:\n",
    "                next_state = 'final'\n",
    "            else:\n",
    "                next_state = 's1'\n",
    "        elif curr_state == 'final':\n",
    "            if curr_input == 2:\n",
    "                next_state == 'final'\n",
    "            else:\n",
    "                next_state == 'init'\n",
    "        else:\n",
    "            raise Exception(f'State \"{curr_state}\" is not defined.')\n",
    "        # print(next_state)\n",
    "        return next_state\n",
    "\n",
    "# Some tests\n",
    "e0 = Event0()\n",
    "e1 = Event1()\n",
    "sequence = [1,2,1,3,0]\n",
    "result1 = e0.check(sequence)\n",
    "result2 = e1.check(sequence)\n",
    "\n",
    "print(result1, result2)\n",
    "\n",
    "sequence = [2,2,0,2,1]\n",
    "result1 = e0.check(sequence)\n",
    "result2 = e1.check(sequence)\n",
    "print(result1, result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Complex Event Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1664783273154,
     "user": {
      "displayName": "LIYING HAN",
      "userId": "12884455188191918408"
     },
     "user_tz": 420
    },
    "id": "WyFfbzn5eGoa",
    "outputId": "7d694549-bd83-4351-f720-6c38e01ebdb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 3, 60])\n",
      "torch.Size([8000])\n",
      "torch.Size([2000, 3, 60])\n",
      "torch.Size([2000])\n",
      "tensor([1, 0, 0,  ..., 1, 1, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "''' Generate complex event training/test set using prmitive activities '''\n",
    "\n",
    "n_event_data = 10000\n",
    "n_event_class = 2\n",
    "event_length = 3\n",
    "\n",
    "''' \n",
    "event_dict[key]: complex event class ID.\n",
    "event_dict[item]: complex event pattern of the corresponding complex event class.\n",
    "\n",
    "In this simple example, \n",
    "    event 0 means \"primitive activity 1 happens before 3\", \n",
    "    event 1 means \"primitive activity 0 happens before 2\"\n",
    "'''\n",
    "event_0 = Event0()\n",
    "event_1 = Event1()\n",
    "event_fsm = [event_0, event_1]\n",
    "\n",
    "idx_list = []\n",
    "for label in range(n_class):\n",
    "    idx = list(np.where(y_prim_data == label)[0])\n",
    "    idx_list.append(idx)\n",
    "\n",
    "x_event = np.empty((0, x_prim_data.shape[1], x_prim_data.shape[2] * event_length))\n",
    "y_event = np.empty((0))\n",
    "\n",
    "while len(x_event) < n_event_data:\n",
    "    # generate a random combination of primitive activities\n",
    "    activity_comb = np.random.choice(n_class, event_length)\n",
    "\n",
    "    for fsm in event_fsm:\n",
    "        if fsm.check(activity_comb) == True:\n",
    "            # label this data\n",
    "            event_label = fsm.label\n",
    "            y_event = np.append(y_event, event_label)\n",
    "            x_event_sub = np.empty((1, x_prim_data.shape[1], 0))\n",
    "            for activity_class in activity_comb:\n",
    "                x_event_sub = np.append(x_event_sub, x_prim_data[np.random.choice(idx_list[activity_class], 1)], axis=-1)\n",
    "\n",
    "            x_event = np.append(x_event, x_event_sub, axis=0)\n",
    "            # currently only consider single label case\n",
    "            break\n",
    "    \n",
    "# shuffle the list\n",
    "temp = list(zip(x_event, y_event))\n",
    "random.shuffle(temp)\n",
    "res1, res2 = zip(*temp)\n",
    "# res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "x_event, y_event = np.array(res1), np.array(res2)\n",
    "\n",
    "# Construct trainig set and test set\n",
    "x_event = x_event.astype(np.float32)\n",
    "y_event = y_event.astype(np.int32)\n",
    "\n",
    "# x_event, y_event = utils.shuffle(x_event, y_event)\n",
    "\n",
    "x_event_train = x_event[:-int(n_event_data/5)]\n",
    "x_event_test = x_event[-int(n_event_data/5):]\n",
    "y_event_train = y_event[:-int(n_event_data/5)]\n",
    "y_event_test = y_event[-int(n_event_data/5):]\n",
    "\n",
    "x_event_train = torch.from_numpy(x_event_train)\n",
    "y_event_train = torch.from_numpy(y_event_train)\n",
    "x_event_test = torch.from_numpy(x_event_test)\n",
    "y_event_test = torch.from_numpy(y_event_test)\n",
    "\n",
    "print(x_event_train.shape)\n",
    "print(y_event_train.shape)\n",
    "print(x_event_test.shape)\n",
    "print(y_event_test.shape)\n",
    "\n",
    "print(y_event_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1664774899089,
     "user": {
      "displayName": "LIYING HAN",
      "userId": "12884455188191918408"
     },
     "user_tz": 420
    },
    "id": "L_vnJI7tRISP",
    "outputId": "5ce20807-8a9d-4c17-eb75-ca8e1fef4c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 3, 60)\n",
      "(2000, 3, 60)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(x_event_train).shape)\n",
    "print(np.array(x_event_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1pcC_ncv393O"
   },
   "outputs": [],
   "source": [
    "'''Process input sequence by a fixed window size'''\n",
    "\n",
    "def predict_sequence(model, x_event, segment_size):\n",
    "    '''\n",
    "    model: the primitive activity classifer\n",
    "    x_event: the complex event data of shape (batch_size, num_channel, sequence_length)\n",
    "    segment_size: the length of the input required by primitive activity classifier\n",
    "    '''\n",
    "    segments_num = int(x_event.shape[-1] / segment_size)\n",
    "    logits_sequence = []\n",
    "\n",
    "    for i in range(segments_num):\n",
    "        x_event_temp = x_event[:,:,i * segment_size:(i + 1) * segment_size]\n",
    "        output = model(x_event_temp)\n",
    "        logits_sequence.append(output)\n",
    "    \n",
    "    return logits_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define FSM constraints in PYLON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uAOi7nxWUkcP"
   },
   "outputs": [],
   "source": [
    "'''Define constraint function and Pylon constraint loss'''\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from pylon.constraint import constraint\n",
    "from pylon.brute_force_solver import SatisfactionBruteForceSolver\n",
    "# from pylon.brute_force_solver import *\n",
    "\n",
    "def complex_event(*logits_sequence, **kwargs):\n",
    "    '''\n",
    "    logits_sequence: a sequence of logits tensors returned by the primitive classifier\n",
    "    kwargs['event_label']: ground truth complex event label\n",
    "    kwargs['event_fsm']: a list of finite state machines for corresponding complex event class\n",
    "    '''\n",
    "\n",
    "    for fsm in kwargs['event_fsm']:\n",
    "        if fsm.check(logits_sequence) == True: # (torch,torch,torch) == (int,int,int) in pylon constraints\n",
    "            return fsm.label == kwargs['event_label']\n",
    "    return False\n",
    "\n",
    "complex_event_cons = constraint(complex_event, SatisfactionBruteForceSolver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vNIarV4fsach"
   },
   "outputs": [],
   "source": [
    "model = CRNN(n_class)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1664781550564,
     "user": {
      "displayName": "LIYING HAN",
      "userId": "12884455188191918408"
     },
     "user_tz": 420
    },
    "id": "3B4OAifXtlHv",
    "outputId": "2d0c8aea-3f28-4780-8136-4d6abd030cdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test data: 24 %\n",
      "[512.0, 485.0, 512.0, 491.0]\n",
      "Accuracy of activity  0 : 19 %\n",
      "Accuracy of activity  1 :  9 %\n",
      "Accuracy of activity  2 :  3 %\n",
      "Accuracy of activity  3 : 62 %\n"
     ]
    }
   ],
   "source": [
    "predict, acc = evaluate(model, x_prim_test, y_prim_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "executionInfo": {
     "elapsed": 542,
     "status": "error",
     "timestamp": 1664783472928,
     "user": {
      "displayName": "LIYING HAN",
      "userId": "12884455188191918408"
     },
     "user_tz": 420
    },
    "id": "iZYmtAnhtsCz",
    "outputId": "198cab53-077a-4683-a5c7-78aa3d683689"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/8000 [00:00<?, ?it/s]../pylon/brute_force_solver.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  else torch.tensor(data=self.cond(*sample,**kwargs), dtype=torch.bool) for sample in samples ])\n",
      "  1%|▍                                        | 95/8000 [00:02<02:54, 45.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m logits_sequence \u001b[38;5;241m=\u001b[39m predict_sequence(model, x_event_train[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), SEGMENT_SIZE)\n\u001b[1;32m     17\u001b[0m closs \u001b[38;5;241m=\u001b[39m complex_event_cons(\u001b[38;5;241m*\u001b[39mlogits_sequence, event_label\u001b[38;5;241m=\u001b[39my_event_train[i], event_fsm\u001b[38;5;241m=\u001b[39mevent_fsm)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mcloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     dampner = closs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from examples.plothelper import PlotHelper\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "plot_loss = PlotHelper()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "      \n",
    "    # train\n",
    "    for i in tqdm(range(len(x_event_train))):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # print(len(x_event_train))\n",
    "        logits_sequence = predict_sequence(model, x_event_train[i].unsqueeze(0), SEGMENT_SIZE)\n",
    "        \n",
    "        closs = complex_event_cons(*logits_sequence, event_label=y_event_train[i], event_fsm=event_fsm)\n",
    "\n",
    "        closs.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        #     dampner = closs\n",
    "        dampner = closs.detach()\n",
    "        plot_loss.add(cons_loss = dampner)\n",
    "\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            evaluate(model, x_prim_test, y_prim_test)\n",
    "        \n",
    "    evaluate(model, x_prim_test, y_prim_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b2C4g2ntr1K"
   },
   "outputs": [],
   "source": [
    "plot_loss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "05332edfd481c973894cc03393e2e880d68b6aff777caafc6fa46463106a6a2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
